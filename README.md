# Classification-and-Regression-Task
PLA &amp; LR &amp; NB &amp; KNN &amp; Linear Regression

       使用感知机算法、逻辑回归、朴素贝叶斯、KNN、线性回归解决有监督学习的分类和回归任务。


     - 使用PLA、LR、knn、朴素贝叶斯实现二分类，得到最高准确率为88.3。
     - 使用PLA、LR、朴素贝叶斯、knn实现五分类，得到最高准确率为43.3。
     - 使用线性回归模型、对Tag处理，对特征进行提取，进行特征工程处理，实现最高相关系数为0.6569。
          
              
          
      1. 数据处理

      分类：

      - 对数据单词的大小写、标点符号进行处理，去掉多余的标点符号，将所有单词统一换成小写，去掉没用的低频词，比如“the”。
      - 使用word2vec对单词进行处理，然后对每个句子的单词进行加权求和，实现从高维度降为低维度，提高模型训练速度，但是由于后期模型训练效果很差，估计是因为数据准确性丢失太多，因此后来放弃这种数据处理方法。
      - 使用tf-idf矩阵，在使用这种方式时，由于在模型中使用效果不好，而且字典化比较麻烦，因此到后面弃用这种表示方法。
      - 使用onehot矩阵，但是由于词向量达到80000+维，因此建立这么大的矩阵不理想，难以继续后面的调参，而且稀疏矩阵很多位置都为0，并没有太多意义，因此使用字典存储onehot，简化矩阵表示，使用梯度下降法时，只需要更新每个句子的出现词的对应的权值就好，训练速度快，没有丢失数据准确性，充分利用数据，准确率高，方便后面的调参。



      回归：

      - 对Tag进行处理，将Tag出现词建立一个onehot矩阵，同时考虑到某些词对模型的提升效果不好，可能会下降，因此我选择遍历Tag每一个词的onehot向量，最后选出效果最好的22个词，即22个特征向量，加到原有的6个特征向量组成新的矩阵，相关系数提升0.01。
      - 经过分析比较，发现原有的6个特征列中Average_Score、Review_Total_Negative_Word_Counts、Review_Total_Positive_Word_Counts三个特征列对模型的训练非常重要，并且还没有充分处理，因此选择对三者进行特征交集处理，比如Review_Total_Negative_Word_Counts，Review_Total_Positive_Word_Counts两列分别代表负面、正面的词的个数，因此可以将两列中每一行负面和正面的单词数相加得出一个总数，分别求出当前行负面单词数和正面单词数占总数的比率，从而得出负面单词率和正面单词率的两个特征列，依次类推，适当对三列特征进行各种交集运算处理，充分利用有用的特征，实验证明，非常有效提高相关系数。（实际上，提升准确率和相关系数在于模型和数据特征处理，如果数据处理得好，即使是简单的模型也能得到很好效果。）



      2. PLA

      - PLA基于线性回归的方法，使用w向量权值与每一行句子的句子onehot向量x相乘，得出计算结果y，如果y大于等于0，预测为1，如果y小于0，预测为0，由于使用onehot矩阵时，每次w与x相乘时，x中所有为0的位置与对应的w向量的权值相乘时为0，没有计入结果y中，只有x中为1的对应权值才累加到y中，因此为了提升训练速度，使用字典存储onehot，因此在计算y时，每次选择句子中出现的词的向量权值进行累加，得到y，效果跟使用onehot矩阵一样，但效率高。
      - 由计算结果y得到预测结果y1后，与正确结果比较，如果相同则不用更新，否则需要进行更新，对句子中每一个词的权值w1进行更新，引入一个学习率rate，调整更新的幅度，使用梯度下降法更新权值，每次迭代遍历所有句子，判断每次遍历预测结果是否正确，正确则继续，否则进行更新，直到梯度下降条件终止。
      - 两种梯度下降方法：
        - 使用不变的学习率rate，每一次迭代计算训练数据的准确率，直到某一次准确率达到1.0，停止训练，然而这样往往会过拟合，因此最终测试准确率在0.873。
        - 使用可变的学习率rate，每一次迭代对rate=rate/迭代数计算，进行30次迭代，训练的梯度步伐会越来越小，更好达到最终点，训练准确率只有90.03，但是测试准确率上升到0.883。


      3. 逻辑回归

      - 逻辑回归同PLA一样，使用基于线性回归和onehot字典，将w与句子向量x相乘得出计算结果y后，使用sigmoid激活函数得出预测结果y1，根据逻辑回归的梯度下降公式推导，计算出实际值与预测结果y1的误差error，对句子中的每一个词的权值w1进行更新。
      - 使用两种梯度下降方法：
        - 不变学习率rate，每次迭代计算训练准确率，只有预测不准确进行更新，直到预测准确率达到0.99，停止训练，与PLA一样，这种方法会过拟合，测试准确率只有0.853。
        - 可变学习率rate，每次迭代rate=rate/迭代数，30次迭代，所有数据都更新，训练准确率只有90.66，测试准确率上升为0.878。



      4. 朴素贝叶斯

      - 基于概率公式，以二分类为例，首先计算label值为1的概率p_1，用p1_dic字典存储label值为1对应的句子中各个词出现的次数，用p1_total统计label值为1对应的句子的单词总数，针对每一行句子，判断该句子的label是否为1，如果是1，则该句子中的各个词在p1_dic对应的出现次数+1，该句子的单词总数叠加到p1_total中。统计完毕后，将p1_dic中各个词的出现次数除以p1_total作为各个词在label值为1的所有单词中的频率。
      - 为了避免结果出现下溢，使用对数存储上一点提到的频率，方便在下面将相乘改为相加。
      - 同样，计算出label值为0的相应结果。
      - 在预测时，针对当前句子，计算label为1概率时，将句子中各个词在p1_dic中对应的value值相加，最后在加上math.log(p_1)，得到label为1的概率，同理得到label为0的概率，根据概率的大小比较得出预测结果，五分类也是同样的方法。



      5. 线性回归

      - 首先进行特征处理，对原有特征进行特征工程处理，再加上偏置单元，扩展为31列特征，对Tag进行onehot处理，经过比较，去除没用的特征词，最后选出22列特征，最终扩展为53列特征。
      - 使用线性回归，对特征矩阵的每一行x，与向量w相乘，得到预测结果y，求出误差error=true_y-y，使用梯度下降法，学习率rate随着迭代次数增加而下降，迭代次数设为30，不断更新优化w，直到效果最好。



      6. knn

      分类：

      - 使用word2Vec处理数据降维为500维，使用欧式距离为度量方式，设置k值为150，选出150个与预测句子欧式距离最近的句子，对句子进行加权处理，同时给距离一个惩罚系数，距离越大，惩罚系数越高，距离越短，权值越大，重要性越强，选出权值最高的label值。

      回归：

      - 使用余弦相似度为度量方式，设置k值为150，选出150个与预测句子余弦相似度最大的句子，针对余弦相似度进行重要性处理，数值越大，越重要，其权值越重要，对句子的label值进行加权处理，得出最终的预测结果。
  

            
